{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Trump Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will attempt to perform some analysis on Donald Trump's tweets. I hope to learn how to access Twitter's API to scrape tweet data, as well as some basic natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set up access to Twitter API \n",
    "consumer_key = 'SsPIsxuj0sGUbSiSPVVUhqY9b'\n",
    "consumer_secret = 'vQi4cbKyYAnjYjuMFZ79g9i0MJFrgUofkZ7UfDyS2MFMNMYWsU'\n",
    "access_token = '868623131799412736-cyBXAkrU6fSWWWaIT97st2DUiyBCQvW'\n",
    "access_secret = 'Ei2jdovTshu9DVW7lbUsGv8CcbdrFKjC7meUSzBDHDHtb'\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since the Twitter API only returns tweets of one page at a time, we will have to iterate through the pages to grab the entire timeline. This code is adapted from https://gist.github.com/yanofsky/5436496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize a list to hold all the tweepy Tweets\n",
    "alltweets = []\n",
    "\n",
    "#make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "new_tweets = api.user_timeline(screen_name = 'realDonaldTrump',count=200,include_rts = True)\n",
    "\n",
    "#save most recent tweets\n",
    "alltweets.extend(new_tweets)\n",
    "\n",
    "#save the id of the oldest tweet less one\n",
    "oldest = alltweets[-1].id - 1\n",
    "\n",
    "#keep grabbing tweets until there are no tweets left to grab\n",
    "while len(new_tweets) > 0:\n",
    "    print(\"getting tweets before %s\" % (oldest))\n",
    "\n",
    "    #all subsiquent requests use the max_id param to prevent duplicates\n",
    "    new_tweets = api.user_timeline(screen_name = 'realDonaldTrump',count=200,max_id=oldest,include_rts = False)\n",
    "\n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "\n",
    "    #update the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "\n",
    "    print(\"...%s tweets downloaded so far\" % (len(alltweets)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so now we have a list of all the tweet objects. Each tweet object has a wealth of associated metadata, which we can explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dir(alltweets[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "At this point, I really want to use NLTK's VADER sentiment analysis package to calculate a sentiment score for each tweet. The algorithm calculates a compound \"polarity\" score for each tweet - that is, it analyzes words that are commonly associated with positive or negative sentiments, and assigns a weighted average of sorts to the tweet. It might be interesting to plot the sentiment of each tweet over time and see if we can find any interesting results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since for now the only information we need is the tweet text itself, let's create a dictionary of the tweet content with its tweet ID so we can reference other metadata later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweetcontent = {}\n",
    "for tweet in alltweets: \n",
    "    tweetcontent[tweet.id] = tweet.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, now we have a dictionary of tweet IDs and the relevant tweet content. Let's throw this into VADER and see what we find..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "for key in tweetcontent:\n",
    "    print(tweetcontent[key])\n",
    "    ss = sid.polarity_scores(tweetcontent[key])\n",
    "    for k in sorted(ss):\n",
    "        print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
